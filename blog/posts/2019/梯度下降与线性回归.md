---
title: 梯度下降与线性回归
author: shenfq
date: 2019/01/28
categories:
- 机器学习
tags:
- 机器学习
- 算法
- 数学
---

# 梯度下降与线性回归

## 基本概念

> **wikipedia：** 梯度下降法是一个一阶最优化算法，通常也称为最速下降法。要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。

梯度下降法是机器学习中最常用的优化方法之一，主要作用是求解目标函数的极小值。基本原理就是让目标函数沿着某个方向去搜索极小值，而这个方向就是梯度下降的方向，如果搜索极大值，就是沿着梯度上升方向。

![image](https://file.shenfq.com/19-01-28/1.png)


<!-- more -->


## 什么是梯度

那么现在就有一个问题，梯度是怎么来的？

梯度是微积分中一个很重要的概念。在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向

这个需要我们一步步进行引申，首先我们都知道导数是函数在某点的斜率的变化，代表了自变量的瞬时变化率。在涉及到两个自变量的时候，函数图像从曲线来到了曲面，但是曲面有无数条切线。这个时候就有了偏导数的概念，表示多元函数沿着坐标轴的变化率。

![image](https://file.shenfq.com/19-01-28/2.png)

$f_x(x, y)$ 或者 $\frac{\partial f}{\partial x}$ 表示沿着x轴方向的变化率

$f_y(x, y)$ 或者 $\frac{\partial f}{\partial y}$ 表示沿着y轴方向的变化率

这样依然不够，偏导数也只能表示出坐标轴方向的变化率，如果要表示指定方向的变化率，就需要引入方向导数的概念。

![image](https://file.shenfq.com/19-01-28/3.png)

假设有这样一个函数`z = f(x, y)`，现在存在一个点 $(x_0, y_0)$ ，这个点在函数图像上有无数个切线，现在我们再引入一个方向向量 u，我们可以表示点 $(x_0, y_0)$ 在方向向量 u 方向的斜率的。

![image](https://file.shenfq.com/19-01-28/4.png)

上图中蓝色的点就是$(x_0, y_0)$，紫色的射线就是方向向量，那么方向向量可以表示为$ (x_0 + tcos\alpha, y_0 + tsin\alpha) $，其中 $\alpha$ 表示向量与x的夹角，t表示方向向量的长度。那么函数在点 $(x_0, y_0)$ 沿着方向向量u的变化率可以表示为：$ f_x(x_0, y_0)cos\alpha + f_y(x_0, y_0)sin\alpha $。下面看看方向导数的表达式：

$$
D_u f(x, y) = \frac{\partial f}{\partial x} \cos\theta + \frac{\partial f}{\partial y} \sin\theta
$$

现在假设，$A = (\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y})  $, $ I = (\cos\theta, \sin\theta) $，那么可以得到下面表达式：

$$
D_u f(x, y) = A \cdot I = |A| |I| cos\alpha
$$

那么想要 $D_u f(x, y)$ 取到最大值，$\alpha$（向量A和向量I直接的夹角）为0度时，$cos\alpha$ 等于1。也就是当向量I与向量A平行的时候，方向导数最大。

现在我们就能得到梯度的表达式：

$$
gradf(x,y) = \nabla f(x, y) = (\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y})
$$

现在我们得到结论：`梯度向量是方向导数最大的方向，也就是曲面上最陡峭的方向。` 因为梯度的方向是函数在给定点上升最快的方向，所以梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。我们只要沿着梯度下降的方向一直走，就能走到局部的最低点。


## 线性回归

现在使用梯度下降法来做一个简单的线性回归。线性回归是一种回归分析，简单的说就是在一些已知的 (x, y) 坐标点中，统计出尽量与所有点都靠近的函数`h(x)`。然后使用抽象出的函数对输入的的x预测出新的y。

![image](https://file.shenfq.com/19-01-28/5.png)

假设上图是同一地段，房屋面积与房价的关系。现在我们需要通过梯度下降法得到一个预测函数（θ 为回归系数）：

$$
h_{\theta}(x) = \theta_0 x + \theta_1
$$

那么我们如何评估这个预测函数是否符合预期呢，这里可以使用最小均方差里描述误差：

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} ( h_\theta(x_i) - y_i )^2
$$

这里的 m 表示我们所有的已知坐标点的数量，用 h 函数预测的 y 减去真实的 y 求的方差，通常误差评估的函数在机器学习中被成为代价函数。现在我们来使用梯度下降来调节 θ：

$$
\theta_j = \theta_j - \alpha \nabla f(\theta) = \theta_j - \alpha  \frac{\partial }{\partial \theta_j} J(\theta)
$$

简单来说就是让 θ 每次减去 α 乘以梯度，让函数往最小误差偏移，这里的α成为学习率。其作用是限制梯度方向的速率，如果步子太大很容易迈过最小值，而步子太小又会减缓寻找最小值的速率。在实际编程中，学习率可以以 3 倍，10 倍这样进行取值尝试。

![image](https://file.shenfq.com/19-01-28/6.png)

将代价函数代入到梯度下降函数中，可以得到：

$$
\theta_j = \theta_j - \alpha  \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i) 
$$

接下来要做的就是把所有已知的的点代入进行计算，最后得到预测函数。实际的机器学习训练中，不会使用全部的数据进行训练，一般会取出70%的数据进行训练，剩余的30%的数据用来进行测试，测试训练模型的准确率如何。


## 总结

其中用到的微积分知识比较多，大学学过后很久都没有用过了，还是要多复习。
另外梯度下降是机器学习中常用的优化方式，在深度学习中也有应用，需要好好掌握。使用梯度下降最重要的就是找到一个合适的学习率，这样才能高效的完成训练。
