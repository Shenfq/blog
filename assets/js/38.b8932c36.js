(window.webpackJsonp=window.webpackJsonp||[]).push([[38],{447:function(a,t,_){"use strict";_.r(t);var p=_(2),v=Object(p.a)({},(function(){var a=this,t=a._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h2",{attrs:{id:"基本概念"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#基本概念"}},[a._v("#")]),a._v(" 基本概念")]),a._v(" "),t("blockquote",[t("p",[t("strong",[a._v("wikipedia：")]),a._v(" 梯度下降法是一个一阶最优化算法，通常也称为最速下降法。要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。")])]),a._v(" "),t("p",[a._v("梯度下降法是机器学习中最常用的优化方法之一，主要作用是求解目标函数的极小值。基本原理就是让目标函数沿着某个方向去搜索极小值，而这个方向就是梯度下降的方向，如果搜索极大值，就是沿着梯度上升方向。")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://file.shenfq.com/19-01-28/1.png",alt:"image"}})]),a._v(" "),t("h2",{attrs:{id:"什么是梯度"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#什么是梯度"}},[a._v("#")]),a._v(" 什么是梯度")]),a._v(" "),t("p",[a._v("那么现在就有一个问题，梯度是怎么来的？")]),a._v(" "),t("p",[a._v("梯度是微积分中一个很重要的概念。在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向")]),a._v(" "),t("p",[a._v("这个需要我们一步步进行引申，首先我们都知道导数是函数在某点的斜率的变化，代表了自变量的瞬时变化率。在涉及到两个自变量的时候，函数图像从曲线来到了曲面，但是曲面有无数条切线。这个时候就有了偏导数的概念，表示多元函数沿着坐标轴的变化率。")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://file.shenfq.com/19-01-28/2.png",alt:"image"}})]),a._v(" "),t("p",[a._v("$f_x(x, y)$ 或者 $\\frac{\\partial f}{\\partial x}$ 表示沿着x轴方向的变化率")]),a._v(" "),t("p",[a._v("$f_y(x, y)$ 或者 $\\frac{\\partial f}{\\partial y}$ 表示沿着y轴方向的变化率")]),a._v(" "),t("p",[a._v("这样依然不够，偏导数也只能表示出坐标轴方向的变化率，如果要表示指定方向的变化率，就需要引入方向导数的概念。")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://file.shenfq.com/19-01-28/3.png",alt:"image"}})]),a._v(" "),t("p",[a._v("假设有这样一个函数"),t("code",[a._v("z = f(x, y)")]),a._v("，现在存在一个点 $(x_0, y_0)$ ，这个点在函数图像上有无数个切线，现在我们再引入一个方向向量 u，我们可以表示点 $(x_0, y_0)$ 在方向向量 u 方向的斜率的。")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://file.shenfq.com/19-01-28/4.png",alt:"image"}})]),a._v(" "),t("p",[a._v("上图中蓝色的点就是$(x_0, y_0)$，紫色的射线就是方向向量，那么方向向量可以表示为$ (x_0 + tcos\\alpha, y_0 + tsin\\alpha) $，其中 $\\alpha$ 表示向量与x的夹角，t表示方向向量的长度。那么函数在点 $(x_0, y_0)$ 沿着方向向量u的变化率可以表示为：$ f_x(x_0, y_0)cos\\alpha + f_y(x_0, y_0)sin\\alpha $。下面看看方向导数的表达式：")]),a._v(" "),t("p",[a._v("$$\nD_u f(x, y) = \\frac{\\partial f}{\\partial x} \\cos\\theta + \\frac{\\partial f}{\\partial y} \\sin\\theta\n$$")]),a._v(" "),t("p",[a._v("现在假设，$A = (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})  $, $ I = (\\cos\\theta, \\sin\\theta) $，那么可以得到下面表达式：")]),a._v(" "),t("p",[a._v("$$\nD_u f(x, y) = A \\cdot I = |A| |I| cos\\alpha\n$$")]),a._v(" "),t("p",[a._v("那么想要 $D_u f(x, y)$ 取到最大值，$\\alpha$（向量A和向量I直接的夹角）为0度时，$cos\\alpha$ 等于1。也就是当向量I与向量A平行的时候，方向导数最大。")]),a._v(" "),t("p",[a._v("现在我们就能得到梯度的表达式：")]),a._v(" "),t("p",[a._v("$$\ngradf(x,y) = \\nabla f(x, y) = (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})\n$$")]),a._v(" "),t("p",[a._v("现在我们得到结论："),t("code",[a._v("梯度向量是方向导数最大的方向，也就是曲面上最陡峭的方向。")]),a._v(" 因为梯度的方向是函数在给定点上升最快的方向，所以梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。我们只要沿着梯度下降的方向一直走，就能走到局部的最低点。")]),a._v(" "),t("h2",{attrs:{id:"线性回归"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#线性回归"}},[a._v("#")]),a._v(" 线性回归")]),a._v(" "),t("p",[a._v("现在使用梯度下降法来做一个简单的线性回归。线性回归是一种回归分析，简单的说就是在一些已知的 (x, y) 坐标点中，统计出尽量与所有点都靠近的函数"),t("code",[a._v("h(x)")]),a._v("。然后使用抽象出的函数对输入的的x预测出新的y。")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://file.shenfq.com/19-01-28/5.png",alt:"image"}})]),a._v(" "),t("p",[a._v("假设上图是同一地段，房屋面积与房价的关系。现在我们需要通过梯度下降法得到一个预测函数（θ 为回归系数）：")]),a._v(" "),t("p",[a._v("$$\nh_{\\theta}(x) = \\theta_0 x + \\theta_1\n$$")]),a._v(" "),t("p",[a._v("那么我们如何评估这个预测函数是否符合预期呢，这里可以使用最小均方差里描述误差：")]),a._v(" "),t("p",[a._v("$$\nJ(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} ( h_\\theta(x_i) - y_i )^2\n$$")]),a._v(" "),t("p",[a._v("这里的 m 表示我们所有的已知坐标点的数量，用 h 函数预测的 y 减去真实的 y 求的方差，通常误差评估的函数在机器学习中被成为代价函数。现在我们来使用梯度下降来调节 θ：")]),a._v(" "),t("p",[a._v("$$\n\\theta_j = \\theta_j - \\alpha \\nabla f(\\theta) = \\theta_j - \\alpha  \\frac{\\partial }{\\partial \\theta_j} J(\\theta)\n$$")]),a._v(" "),t("p",[a._v("简单来说就是让 θ 每次减去 α 乘以梯度，让函数往最小误差偏移，这里的α成为学习率。其作用是限制梯度方向的速率，如果步子太大很容易迈过最小值，而步子太小又会减缓寻找最小值的速率。在实际编程中，学习率可以以 3 倍，10 倍这样进行取值尝试。")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://file.shenfq.com/19-01-28/6.png",alt:"image"}})]),a._v(" "),t("p",[a._v("将代价函数代入到梯度下降函数中，可以得到：")]),a._v(" "),t("p",[a._v("$$\n\\theta_j = \\theta_j - \\alpha  \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x_i) - y_i)\n$$")]),a._v(" "),t("p",[a._v("接下来要做的就是把所有已知的的点代入进行计算，最后得到预测函数。实际的机器学习训练中，不会使用全部的数据进行训练，一般会取出70%的数据进行训练，剩余的30%的数据用来进行测试，测试训练模型的准确率如何。")]),a._v(" "),t("h2",{attrs:{id:"总结"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[a._v("#")]),a._v(" 总结")]),a._v(" "),t("p",[a._v("其中用到的微积分知识比较多，大学学过后很久都没有用过了，还是要多复习。\n另外梯度下降是机器学习中常用的优化方式，在深度学习中也有应用，需要好好掌握。使用梯度下降最重要的就是找到一个合适的学习率，这样才能高效的完成训练。")])])}),[],!1,null,null,null);t.default=v.exports}}]);